<p align="center">
  <img src="https://img.shields.io/badge/v1.2.3-stable-blueviolet?style=for-the-badge" alt="Version">
  <img src="https://img.shields.io/badge/TypeScript-5.7-3178c6?style=for-the-badge&logo=typescript&logoColor=white" alt="TypeScript">
  <img src="https://img.shields.io/badge/Node.js-22+-339933?style=for-the-badge&logo=nodedotjs&logoColor=white" alt="Node.js">
  <img src="https://img.shields.io/badge/License-MIT-f5c542?style=for-the-badge" alt="MIT License">
  <img src="https://img.shields.io/badge/Tests-4350%2B_passing-22c55e?style=for-the-badge" alt="Tests">
</p>

<h1 align="center">ğŸ¥¥ Corbat-Coco</h1>

<p align="center">
  <strong>The open-source coding agent that iterates on your code until it's actually production-ready.</strong>
</p>

<p align="center">
  <em>Generate â†’ Test â†’ Measure â†’ Fix â†’ Repeat â€” autonomously.</em>
</p>

---

## Why Coco?

Most AI coding tools generate code and hand it to you. If something breaks â€” tests fail, types don't match, a security issue slips in â€” that's your problem.

Coco takes a different approach. After generating code, it **runs your tests, measures quality across 12 dimensions, diagnoses what's wrong, and fixes it** â€” in a loop, autonomously â€” until the code actually meets a quality bar you define.

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Generate â”‚ â”€â”€â–º â”‚   Test   â”‚ â”€â”€â–º â”‚ Measure  â”‚ â”€â”€â–º â”‚   Fix    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
                                              Score < 85? â”‚ â”€â”€â–º Loop back
                                              Score â‰¥ 85? â”‚ â”€â”€â–º Done âœ…
```

This is the **Quality Convergence Loop** â€” Coco's core differentiator.

---

## Quick Start

```bash
npm install -g @corbat-tech/coco
coco                        # Opens interactive REPL â€” guided setup on first run
```

That's it. Coco walks you through provider configuration on first launch.

```bash
# Or use it directly:
coco "Add a REST API endpoint for user authentication with tests"
```

---

## What You Can Do

Coco works from the interactive REPL (`coco`). You can use **slash commands** or just **talk naturally** â€” Coco understands both.

### Slash Commands

| Command | What it does | Example |
|---------|-------------|---------|
| `/help` | Show available commands and usage | `/help review` |
| `/status` | Project status, git info, session stats | `/status` |
| `/review` | Code review with severity-rated findings | `/review --base main` |
| `/diff` | Visual diff with syntax highlighting | `/diff --staged` |
| `/ship` | Full release pipeline: review â†’ test â†’ lint â†’ branch â†’ version â†’ commit â†’ PR â†’ CI â†’ merge | `/ship --minor` |
| `/compact` | Reduce context when conversation gets long | `/compact` |
| `/clear` | Clear conversation history | `/clear` |

### Natural Language

You don't need to memorize commands. Just describe what you want:

| What you say | What happens |
|-------------|-------------|
| "review the code" / "revisa el cÃ³digo" | Runs `/review` |
| "let's ship it" / "publica los cambios" | Runs `/ship` |
| "how are we doing?" / "cÃ³mo va?" | Runs `/status` |
| "create a PR" / "crea un pull request" | Runs `/ship` |
| "show me the diff" / "muÃ©strame los cambios" | Runs `/diff` |
| "help" / "ayuda" | Runs `/help` |

### `/ship` â€” Release Pipeline

The most powerful command. Orchestrates the entire release flow in one step:

```
/ship                          # Full pipeline (10 steps)
/ship --skip-tests             # Skip test step
/ship --draft                  # Create draft PR
/ship --patch                  # Force patch version bump
/ship --minor                  # Force minor version bump
/ship --major                  # Force major version bump
/ship --no-version             # Skip version bumping
/ship -m "feat: add auth"     # Pre-set commit message
```

Pipeline: **Preflight â†’ Review â†’ Tests â†’ Lint â†’ Branch â†’ Version â†’ Commit â†’ PR â†’ CI â†’ Merge & Release**

Each step is interactive â€” Coco asks before proceeding when decisions are needed. Press `Ctrl+C` at any point to cancel safely.

---

## What Coco Does Well

### Quality Convergence Loop

Coco doesn't just generate code â€” it iterates until quality converges:

| Iteration | Score | What happened |
|:---------:|:-----:|---------------|
| 1 | 52 | Code generated â€” 3 tests failing, no error handling |
| 2 | 71 | Tests fixed, security vulnerability found |
| 3 | 84 | Security patched, coverage improved to 82% |
| 4 | 91 | All green â€” quality converged âœ… |

The quality bar is yours to set:

```bash
coco build --min-quality 90          # Per-run override
coco config set quality.minScore 90  # Persist in project config
```

Default is **85** (senior-level). You can also configure max iterations, convergence threshold, coverage targets, and security requirements â€” see `coco config init`.

### 12-Dimension Quality Scoring

Every iteration measures your code across 12 dimensions using real static analysis:

| Dimension | How it's measured |
|-----------|-------------------|
| Test Coverage | c8/v8 instrumentation |
| Security | Pattern matching + optional Snyk |
| Complexity | Cyclomatic complexity via AST parsing |
| Duplication | Line-based similarity detection |
| Correctness | Test pass rate + build verification |
| Style | oxlint / eslint / biome integration |
| Documentation | JSDoc coverage analysis |
| Readability | AST: naming quality, function length, nesting |
| Maintainability | AST: file size, coupling, function count |
| Test Quality | Assertion density, edge case coverage |
| Completeness | Export density + test file coverage |
| Robustness | Error handling pattern detection |

> **Transparency note**: 7 dimensions use instrumented measurements. 5 use heuristic-based static analysis. We label which is which â€” no black boxes.

### Multi-Provider Support

Bring your own API keys. Coco works with:

| Provider | Auth | Models |
|----------|------|--------|
| **Anthropic** | API key / OAuth PKCE | Claude Opus, Sonnet, Haiku |
| **OpenAI** | API key | GPT-5.3 Codex, GPT-4.1, o4-mini |
| **Google** | API key / gcloud ADC | Gemini 3, 2.5 Pro/Flash |
| **Ollama** | Local | Any local model (8-24GB RAM) |
| **LM Studio** | Local | Any GGUF model (8-32GB RAM) |
| **Moonshot** | API key | Kimi models |

### Multi-Agent Architecture

Six specialized agents with weighted-scoring routing:

- **Researcher** â€” Explores, analyzes, maps the codebase
- **Coder** â€” Writes and edits code (default route)
- **Tester** â€” Generates tests, improves coverage
- **Reviewer** â€” Code review, quality auditing
- **Optimizer** â€” Refactoring and performance
- **Planner** â€” Architecture design, task decomposition

Coco picks the right agent for each task automatically. When confidence is low, it defaults to the coder â€” no guessing games.

### Interactive REPL

A terminal-first experience with:

- **Ghost-text completion** â€” Tab to accept inline suggestions
- **Slash commands** â€” `/ship`, `/review`, `/diff`, `/status`, `/help`, `/compact`, `/clear`
- **Image paste** â€” `Ctrl+V` to paste screenshots for visual context
- **Intent recognition** â€” Natural language mapped to commands
- **Context management** â€” Automatic compaction when context grows large

### Production Hardening

- **Error recovery** with typed error strategies and exponential backoff
- **Checkpoint/Resume** â€” `Ctrl+C` saves state, `coco resume` picks up where you left off
- **AST validation** â€” Syntax-checks generated code before saving
- **Convergence analysis** â€” Detects oscillation, diminishing returns, and stuck patterns
- **Path sandboxing** â€” Tools can only access files within the project

---

## COCO Methodology

Four phases, each with a dedicated executor:

```
 CONVERGE          ORCHESTRATE         COMPLETE            OUTPUT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gather   â”‚     â”‚ Design       â”‚   â”‚ Execute with â”‚   â”‚ Generate â”‚
â”‚ reqs     â”‚ â”€â”€â–º â”‚ architecture â”‚â”€â”€â–ºâ”‚ quality      â”‚â”€â”€â–ºâ”‚ CI/CD,   â”‚
â”‚ + spec   â”‚     â”‚ + backlog    â”‚   â”‚ convergence  â”‚   â”‚ docs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                         â†‘    â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚ Convergence â”‚
                                    â”‚    Loop     â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

1. **Converge** â€” Understand what needs to be built. Gather requirements, produce a spec.
2. **Orchestrate** â€” Design the architecture, decompose into a task backlog.
3. **Complete** â€” Execute each task with the quality convergence loop.
4. **Output** â€” Generate CI/CD pipelines, documentation, and deployment config.

---

## Use Cases

Coco is designed for developers who want AI assistance with **accountability**:

- **Feature development** â€” Describe what you want, get tested and reviewed code
- **Vibe coding** â€” Explore ideas interactively; Coco handles the quality checks
- **Refactoring** â€” Point at code and say "make this better" â€” Coco iterates until metrics improve
- **Test generation** â€” Improve coverage with meaningful tests, not boilerplate
- **Code review** â€” Get multi-dimensional quality feedback on existing code
- **Learning** â€” See how code quality improves across iterations

---

## Development

```bash
git clone https://github.com/corbat/corbat-coco
cd corbat-coco
pnpm install
pnpm dev          # Run in dev mode (tsx)
pnpm test         # 4,350+ tests via Vitest
pnpm check        # typecheck + lint + test
pnpm build        # Production build (tsup)
```

### Project Structure

```
src/
â”œâ”€â”€ agents/           # Multi-agent coordination + weighted routing
â”œâ”€â”€ cli/              # REPL, commands, input handling, output rendering
â”œâ”€â”€ orchestrator/     # Phase coordinator + state recovery
â”œâ”€â”€ phases/           # COCO phases (converge/orchestrate/complete/output)
â”œâ”€â”€ quality/          # 12 quality analyzers + convergence engine
â”œâ”€â”€ providers/        # 7 LLM providers + OAuth flows
â”œâ”€â”€ tools/            # 20+ tool implementations
â”œâ”€â”€ hooks/            # Lifecycle hooks (safety, lint, format, audit)
â”œâ”€â”€ mcp/              # MCP server for external integration
â””â”€â”€ config/           # Zod-validated configuration system
```

### Technology Stack

| Component | Technology |
|-----------|-----------|
| Language | TypeScript (ESM, strict mode) |
| Runtime | Node.js 22+ |
| Testing | Vitest (4,350+ tests) |
| Linting | oxlint |
| Formatting | oxfmt |
| Build | tsup |
| Schema validation | Zod |

---

## Known Limitations

We'd rather you know upfront:

- **TypeScript/JavaScript first** â€” Other languages have basic support but fewer analyzers
- **CLI-only** â€” No IDE extension yet (VS Code integration is planned)
- **Iteration takes time** â€” The convergence loop adds 2-5 minutes per task. For quick one-line fixes, a simpler tool may be faster
- **Heuristic analyzers** â€” 5 of 12 quality dimensions use pattern-based heuristics, not deep semantic analysis
- **LLM-dependent** â€” Output quality depends on the model you connect. Larger models produce better results
- **Early stage** â€” Actively developed. Not yet battle-tested at large enterprise scale

---

## Contributing

We welcome contributions of all kinds:

- Bug reports and feature requests
- New quality analyzers
- Additional LLM provider integrations
- Documentation and examples
- Real-world usage feedback

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

---

## About

Corbat-Coco is built by [Corbat](https://corbat.tech), a technology consultancy that believes AI coding tools should be transparent, measurable, and open source.

<p align="center">
  <a href="https://github.com/corbat/corbat-coco">GitHub</a> Â· <a href="https://corbat.tech">corbat.tech</a>
</p>

<p align="center"><strong>MIT License</strong> Â· Made by developers who measure before they ship. ğŸ¥¥</p>
